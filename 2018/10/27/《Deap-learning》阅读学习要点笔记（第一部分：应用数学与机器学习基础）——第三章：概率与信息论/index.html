<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="3.1 为什么要使用概率概率可以被看做用于处理不确定性的逻辑扩展。逻辑提供了一套形式化的规则，可以在给定某些命题是真或假的假设下，判断另外一些命题是真的还是假的。  不确定性有3种可能的来源： 被建模系统内在的随机性； 不完全观测； 不完全建模。3.9 常用概率分布     伯努利分布（Bernoulli distribution) 单个二值随机变量的分布。由单个参数 $\phi \in [0,1">
<meta property="og:type" content="article">
<meta property="og:title" content="《Deap learning》阅读学习要点笔记（第一部分：应用数学与机器学习基础）——第三章：概率与信息论">
<meta property="og:url" content="http://yoursite.com/2018/10/27/《Deap-learning》阅读学习要点笔记（第一部分：应用数学与机器学习基础）——第三章：概率与信息论/index.html">
<meta property="og:site_name" content="AgentJames">
<meta property="og:description" content="3.1 为什么要使用概率概率可以被看做用于处理不确定性的逻辑扩展。逻辑提供了一套形式化的规则，可以在给定某些命题是真或假的假设下，判断另外一些命题是真的还是假的。  不确定性有3种可能的来源： 被建模系统内在的随机性； 不完全观测； 不完全建模。3.9 常用概率分布     伯努利分布（Bernoulli distribution) 单个二值随机变量的分布。由单个参数 $\phi \in [0,1">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-10-27T06:15:25.731Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《Deap learning》阅读学习要点笔记（第一部分：应用数学与机器学习基础）——第三章：概率与信息论">
<meta name="twitter:description" content="3.1 为什么要使用概率概率可以被看做用于处理不确定性的逻辑扩展。逻辑提供了一套形式化的规则，可以在给定某些命题是真或假的假设下，判断另外一些命题是真的还是假的。  不确定性有3种可能的来源： 被建模系统内在的随机性； 不完全观测； 不完全建模。3.9 常用概率分布     伯努利分布（Bernoulli distribution) 单个二值随机变量的分布。由单个参数 $\phi \in [0,1">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"right"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/10/27/《Deap-learning》阅读学习要点笔记（第一部分：应用数学与机器学习基础）——第三章：概率与信息论/"/>





  <title>《Deap learning》阅读学习要点笔记（第一部分：应用数学与机器学习基础）——第三章：概率与信息论 | AgentJames</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AgentJames</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            日程表
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      
        
        <li class="menu-item menu-item-something">
          <a href="/something" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            有料
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/27/《Deap-learning》阅读学习要点笔记（第一部分：应用数学与机器学习基础）——第三章：概率与信息论/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lau James">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AgentJames">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">《Deap learning》阅读学习要点笔记（第一部分：应用数学与机器学习基础）——第三章：概率与信息论</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-27T14:14:34+08:00">
                2018-10-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="3-1-为什么要使用概率"><a href="#3-1-为什么要使用概率" class="headerlink" title="3.1 为什么要使用概率"></a>3.1 为什么要使用概率</h2><p>概率可以被看做用于处理不确定性的逻辑扩展。逻辑提供了一套形式化的规则，可以在给定某些命题是真或假的假设下，判断另外一些命题是真的还是假的。</p>
<ul>
<li>不确定性有3种可能的来源：<ol>
<li>被建模系统内在的随机性；</li>
<li>不完全观测；</li>
<li>不完全建模。<h2 id="3-9-常用概率分布"><a href="#3-9-常用概率分布" class="headerlink" title="3.9 常用概率分布"></a>3.9 常用概率分布</h2></li>
</ol>
</li>
</ul>
<ol>
<li><strong>伯努利分布（Bernoulli distribution)</strong><br> 单个二值随机变量的分布。由单个参数 $\phi \in [0,1]$ 控制，$\phi$ 给出了随机变量等于1的概率，基本性质：</li>
</ol>
<ul>
<li>$$P(\text&amp;#123x&amp;#125=1)=\phi$$</li>
<li>$$P(\text&amp;#123x&amp;#125=0)=1-\phi$$ </li>
<li>$$P(\text&amp;#123x=&amp;#125x)=&amp;#123&amp;#123\phi &amp;#125^&amp;#123x&amp;#125&amp;#125&amp;#123&amp;#123(1-\phi )&amp;#125^&amp;#1231-x&amp;#125&amp;#125$$</li>
<li>$$&amp;#123&amp;#123\Epsilon &amp;#125_&amp;#123\text&amp;#123x&amp;#125&amp;#125&amp;#125[\text&amp;#123x&amp;#125]\text&amp;#123=&amp;#125\phi$$</li>
<li>$$Va&amp;#123&amp;#123r&amp;#125_&amp;#123\text&amp;#123x&amp;#125&amp;#125&amp;#125(\text&amp;#123x&amp;#125)=\phi (1-\phi )$$</li>
</ul>
<ol start="2">
<li><strong>高斯分布（Gaussian distribution）</strong> 或  <strong>正态分布（normal distribution）</strong><br>$$\Nu (x;u,&amp;#123&amp;#123\sigma &amp;#125^&amp;#1232&amp;#125&amp;#125)=\sqrt&amp;#123\frac&amp;#1231&amp;#125&amp;#1232\pi &amp;#123&amp;#123\sigma &amp;#125^&amp;#1232&amp;#125&amp;#125&amp;#125&amp;#125\exp (-\frac&amp;#1231&amp;#125&amp;#1232&amp;#123&amp;#123\sigma &amp;#125^&amp;#1232&amp;#125&amp;#125&amp;#125&amp;#123&amp;#123(x-\mu )&amp;#125^&amp;#1232&amp;#125&amp;#125)$$</li>
</ol>
<ul>
<li>均值为$\mu$；标准差为$\sigma$；方差为$\sigma ^2$；</li>
<li>当我们需要对概率密度函数求值时，需要对$\sigma$平方并且取倒数。</li>
<li>当需要经常对不同参数下的概率密度函数求值时，一种更高效的参数化分布的方式时使用参数$\beta \in (0, \infin)$来控制分布的<strong>精度(precision)</strong>（或方差的倒数）：<br>$$\Nu (x;u,&amp;#123&amp;#123\beta &amp;#125^&amp;#123-1&amp;#125&amp;#125)=\sqrt&amp;#123\frac&amp;#123\beta &amp;#125&amp;#1232\pi &amp;#125&amp;#125\exp \left( -\frac&amp;#1231&amp;#125&amp;#1232&amp;#125\beta &amp;#123&amp;#123(x-\mu )&amp;#125^&amp;#1232&amp;#125&amp;#125 \right)$$</li>
<li>采用正态分布在很多应用中都是一个明智的选择。当我们由于缺乏关于某个实数上分布的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选择，主要两个原因：<br>（1）我们想要建模的很多分布的真实情况是比较接近正态分布的。<strong>中心极限定理（central limit theorem）</strong>说明很多独立随机变量的和近似服从正态分布。<br>（2）在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。因此，可以认为正态分布是对模型 <em>加入的先验知识量最少</em> 的分布。</li>
<li>正态分布可以推广到$&amp;#123&amp;#123\mathbb&amp;#123R&amp;#125&amp;#125^&amp;#123n&amp;#125&amp;#125$ ，被称为<strong>多维正态分布（multivariate normal distribution）</strong>。他的参数是一个正定对称矩阵$\Sigma$:<br>$$\Nu (\mathbf&amp;#123x&#125;\mathbf&amp;#123\mu &amp;#125,\mathbf&amp;#123\Sigma &amp;#125)=\sqrt&amp;#123\frac&amp;#1231&amp;#125&amp;#123&amp;#123&amp;#123(2\pi )&amp;#125^&amp;#123n&amp;#125&amp;#125\det (\mathbf&amp;#123\Sigma &amp;#125)&amp;#125&amp;#125\exp \left( -\frac&amp;#1231&amp;#125&amp;#1232&amp;#125&amp;#123&amp;#123(\mathbf&amp;#123x&amp;#125-\mathbf&amp;#123\mu &amp;#125)&amp;#125^&amp;#123T&amp;#125&amp;#125&amp;#123&amp;#123\mathbf&amp;#123\Sigma &amp;#125&amp;#125^&amp;#123-1&amp;#125&amp;#125(\mathbf&amp;#123x&amp;#125-\mathbf&amp;#123\mu &amp;#125) \right)$$</li>
<li>参数$\mathbf&amp;#123\mu &amp;#125$ 仍然表示分布的均值，只不过现在是向量值。参数$\mathbf&amp;#123\Sigma &amp;#125$ 给出了分布的协方差矩阵。同样的，和单变量的情况类似，我们希望对很多不同参数下的概率密度函数多次求值时，协方差矩阵并不是一个很高效的参数化分布的方式，因为对概率密度函数求值时需要对$\mathbf&amp;#123\Sigma &amp;#125$ 求逆，这时可以用<strong>精度矩阵（precision matrix）</strong>$\mathbf&amp;#123\beta&amp;#125$ 进行替代：<br>$$\Nu (\mathbf&amp;#123x&#125;\mathbf&amp;#123\mu &amp;#125,&amp;#123&amp;#123\mathbf&amp;#123\beta &amp;#125&amp;#125^&amp;#123-1&amp;#125&amp;#125)=\sqrt&amp;#123\frac&amp;#123\det (\mathbf&amp;#123\beta &amp;#125)&amp;#125&amp;#123&amp;#123&amp;#123(2\pi )&amp;#125^&amp;#123n&amp;#125&amp;#125&amp;#125&amp;#125\exp \left( -\frac&amp;#1231&amp;#125&amp;#1232&amp;#125&amp;#123&amp;#123(\mathbf&amp;#123x&amp;#125-\mathbf&amp;#123\mu &amp;#125)&amp;#125^&amp;#123T&amp;#125&amp;#125\mathbf&amp;#123\beta &amp;#125(\mathbf&amp;#123x&amp;#125-\mathbf&amp;#123\mu &amp;#125) \right)$$</li>
</ul>
<ol start="3">
<li><strong>指数分布（exponential distribution）</strong><br>在深度学习中，我们经常需要一个在$x=0$点取得边界点（sharp point）的分布，这时可以使用指数分布：<br>$$p(x;\lambda )=\lambda &amp;#123&amp;#1231&amp;#125_&amp;#123x\ge 0&amp;#125&amp;#125\exp (-\lambda x)$$<br>指数分布用指示函数（indicator function）$&amp;#123&amp;#1231&amp;#125_&amp;#123x\ge 0&amp;#125&amp;#125$来使得当$x$取负值时的概率为零。</li>
<li><strong>拉普拉斯分布（Laplace distribution)</strong><br>和指数函数联系紧密，允许在任意一点$\mu$ 处设置概率质量的峰值：<br>$$Laplace(x;\mu ,\gamma )=\frac&amp;#1231&amp;#125&amp;#1232\gamma &amp;#125\exp \left( -\frac&amp;#123\left| x-\mu  \right|&amp;#125&amp;#123\gamma &amp;#125 \right)$$</li>
<li><strong>Dirac分布和经验分布</strong><br>在某些情况下，我们希望概率分布中的所有质量都集中在一个点上。可以通过<strong>Dirac delta函数（Dirac delta function）</strong> $\delta (x)$ 定义概率密度函数来实现：<br>$$p(x)=\delta (x-\mu )$$</li>
</ol>
<ul>
<li>Dirac delta函数被定义成在除了0（$\mu$ 为0）以外的所有点的值都为0，但是积分为1。它不像普通函数一样对$x$ 的每一个值都有一个实数值的输出，它是一种不同类型的数学对象，被称为<strong>广义函数（generalized function）</strong></li>
<li>Dirac 分布经常作为<strong>经验分布（empirical distribution</strong>的一个组成部分出现：<br>$$\overset&amp;#123\hat&amp;#123\ &amp;#125&amp;#125&amp;#123\mathop&amp;#123p&amp;#125&amp;#125\,(x)=\frac&amp;#1231&amp;#125&amp;#123m&amp;#125\sum\limits_&amp;#123i=1&amp;#125^&amp;#123m&amp;#125&amp;#123\delta \left( x-&amp;#123&amp;#123x&amp;#125^&amp;#123(i)&amp;#125&amp;#125 \right)&amp;#125$$<br>经验分布将概率密度$\frac&amp;#1231&amp;#125&amp;#123m&amp;#125$ 赋给$m$ 个点$&amp;#123&amp;#123x&amp;#125^&amp;#123(1)&amp;#125&amp;#125,\ldots ,&amp;#123&amp;#123x&amp;#125^&amp;#123(m)&amp;#125&amp;#125$ 中的每一个，这些点是给定的数据集或者采样的集合。连续型随机变量的经验分布才有应用Dirac delta函数的必要，离散型随机变量的经验分布可以定义成Multinoulli分布（对于每一个可能的输入，其概率可以简单的设为在训练集上那个输入值的<strong>经验频率（empirical frequency）</strong>）</li>
</ul>
<ol start="6">
<li><strong>混合分布（mixture distribution）</strong><br>混合分布由一些组件分布构成。每次实验，样本是由哪个组件分布产生的取决于从一个Multinoulli分布中采样的结果：<br>$$P(x)=\sum\limits_&amp;#123i&amp;#125&amp;#123P(c=i)P(x|c=i)&amp;#125$$<br>这里$P(c)$ 是对各组件的一个Multinoulli分布。</li>
</ol>
<ul>
<li>混合分布以后会经常用到，非常重要。包括<strong>潜变量（latent variable）</strong>等概念</li>
<li>一个非常强大且常见的混合模型是高斯混合分布（<strong>Gaussian Mixture Model</strong>），他的组件$p(x|c=i)$ 是高斯分布，每个组件都有各自的参数（均值和协方差矩阵）。除了均值和协方差之外，高斯混合模型的参数指明了给每个组件$i$ 的<strong>先验概率（prior probability）</strong> $\alpha _i = P(c=i)$</li>
<li>高斯混合模型是概率密度的<strong>万能近似器（universal approximator）</strong>，任何平滑的概率密度都可以用具有足够多组件的高斯混合模型以任意精度来逼近。</li>
</ul>
<hr>
<h2 id="3-10-常用函数的有用性质"><a href="#3-10-常用函数的有用性质" class="headerlink" title="3.10 常用函数的有用性质"></a>3.10 常用函数的有用性质</h2><ol>
<li><strong>logistic sigmoid</strong>函数<br>$$\sigma (x)=\frac&amp;#1231&amp;#125&amp;#1231+\exp (-x)&amp;#125$$<br>logistic sigmoid函数常用来产生Bernoulli分布中的参数$\phi$，因为它的范围是$（0, 1）$，处在$\phi$的有效取值范围内。sigmoid函数在变量取绝对值非常大的正值或者负值时会出现<strong>饱和(saturate)</strong>现象，意味着函数会变得很平，并且对输入的微小改变会变得不敏感。</li>
<li><strong> softplus 函数</strong><br>$$\zeta (x)=\log \left( 1+\exp (x) \right)$$<br>softplus函数可以用来产生正态分布的$\beta$和$\sigma$参数，因为它的范围是$（0, \infin）$，当处理包含sigmoid函数的表达式时，经常出现。函数名来源于它是另外一个函数的平滑（或“软化”）形式：<br>$$&amp;#123&amp;#123x&amp;#125^&amp;#123+&amp;#125&amp;#125=\max (0,x)$$<br>有用的一些性质：</li>
</ol>
<ul>
<li>$\sigma (x)=\frac&amp;#123\exp (x)&amp;#125&amp;#123\exp (x)+\exp (0)&amp;#125$</li>
<li>$\frac&amp;#123d&amp;#125&amp;#123dx&amp;#125\sigma (x)=\sigma (x)\sigma \left( 1-\sigma (x) \right)$  logistics函数的导数可以用自身表示，这个很有利</li>
<li>$1-\sigma (x)=\sigma (-x)$</li>
<li>$\log \sigma (x)=-\zeta (-x)$</li>
<li>$\frac&amp;#123d&amp;#125&amp;#123dx&amp;#125\zeta (x)=\sigma (x)$</li>
<li>$\forall x\in (0,1),&amp;#123&amp;#123\sigma &amp;#125^&amp;#123-1&amp;#125&amp;#125(x)=\log \left( \frac&amp;#123x&amp;#125&amp;#1231-x&amp;#125 \right)$</li>
<li>$\forall x&gt;0,&amp;#123&amp;#123\zeta &amp;#125^&amp;#123-1&amp;#125&amp;#125(x)=\log \left( \exp (x)-1 \right)$</li>
<li>$\zeta (x)=\int_&amp;#123-\infty &amp;#125^&amp;#123x&amp;#125&amp;#123\sigma (y)dy&amp;#125$</li>
<li>$\zeta (x)-\zeta (-x)=x$<br>函数$&amp;#123&amp;#123\sigma &amp;#125^&amp;#123-1&amp;#125&amp;#125(x)$在统计学中被称为<strong>分对数（logit）</strong>，机器学习中很少用到。</li>
</ul>
<hr>
<h2 id="3-11-贝叶斯规则"><a href="#3-11-贝叶斯规则" class="headerlink" title="3.11 贝叶斯规则"></a>3.11 贝叶斯规则</h2><p>经典公式：</p>
<p>$$P(x|y)=\frac&amp;#123P(x)P(y|x)&amp;#125&amp;#123P(y)&amp;#125$$</p>
<p>其中$P(y)$可以使用$P(y)=\sum\nolimits_&amp;#123x&amp;#125&amp;#123P(y|x)P(x)&amp;#125$来计算。</p>
<h2 id="3-13-信息论"><a href="#3-13-信息论" class="headerlink" title="3.13 信息论"></a>3.13 信息论</h2><p>3个重要性质：</p>
<ul>
<li>非常可能发生的事情信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量。</li>
<li>较不可能发生的事件具有更高的信息量。</li>
<li>独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该时投掷一次硬币正面朝上的信息量的两倍。</li>
</ul>
<ol>
<li><p><strong>自信息（self-information)</strong><br>为满足3个性质，定义事件$\text&amp;#123x&amp;#125=x$的<strong>自信息（self-information）为</strong>：<br>$$I(x)=-\log P(x)$$<br>《Deep Learning》使用log来表示自然对数，底数为$e$，$I(x)$单位是<strong>奈特（nats）</strong>。一奈特是以$\frac&amp;#1231&amp;#125&amp;#123e&amp;#125$的概率观测到一个事件时获得的信息量。（其他材料中使用底数为2的对数，单位是<strong>比特（bit）</strong> 或者<strong>香农（shannons）</strong>）</p>
</li>
<li><p><strong>香农熵（Shannon entropy）</strong><br>自信息只处理单个的输出。利用 <strong>香农熵（Shannon entropy）</strong> 来对整个概率分布中的不确定性总量进行量化：<br>$$H(x)=&amp;#123&amp;#123\Epsilon &amp;#125_&amp;#123\text&amp;#123x&amp;#125\sim P&amp;#125&amp;#125\left[ I(x) \right]=-&amp;#123&amp;#123\Epsilon &amp;#125_&amp;#123\text&amp;#123x&amp;#125\tilde&amp;#123\ &amp;#125P&amp;#125&amp;#125\left[ \log P(x) \right]$$<br>也记作$H(P)$</p>
</li>
<li><p><strong>KL散度（Kullback-Leibler（KL）divergence）</strong><br>如果对于一个随机变量$x$有两个单独的概率分布$P(x)$和$Q(x)$，可以使用KL散度来衡量这两个分布的差异：<br>$$&amp;#123&amp;#123D&amp;#125_&amp;#123KL&amp;#125&amp;#125(P||Q)=&amp;#123&amp;#123\Epsilon &amp;#125_&amp;#123\text&amp;#123x&amp;#125\tilde&amp;#123\ &amp;#125P&amp;#125&amp;#125\left[ \log \frac&amp;#123P(x)&amp;#125&amp;#123Q(x)&amp;#125 \right]=&amp;#123&amp;#123\Epsilon &amp;#125_&amp;#123\text&amp;#123x&amp;#125\tilde&amp;#123\ &amp;#125P&amp;#125&amp;#125\left[ \log P(x)-\log Q(x) \right]$$</p>
</li>
</ol>
<ul>
<li>意义：在离散型变量的情况下，KL散度衡量的是，当我们使用一种被设计成能够是的概率分布Q产生的消息的长度最小的编码，发送包含由概率分布$P$产生的符号的消息时，所需要的额外信息量（比特或奈特）</li>
<li>用作分布之间的距离</li>
<li>KL散度和<strong>交叉熵（cross-entropy）</strong> 联系密切：<br>交叉熵：<br>$$H(P,Q)=H(P)+&amp;#123&amp;#123D&amp;#125_&amp;#123KL&amp;#125&amp;#125(P||Q)=-&amp;#123&amp;#123\Epsilon &amp;#125_&amp;#123\text&amp;#123x&amp;#125\tilde&amp;#123\ &amp;#125P&amp;#125&amp;#125\log Q(x)$$<br>和KL散度很像，缺少KL散度左边$\text&amp;#123E&amp;#125_&amp;#123\text&amp;#123x&amp;#125\tilde&amp;#123&amp;#125P&amp;#125logP(x)$<br>-计算这些量时，经常遇到$0log0$这个表达式，信息论中，这个表达式处理为：$\underset&amp;#123x\to 0&amp;#125&amp;#123\mathop&amp;#123\lim &amp;#125&amp;#125\,x\log x=0$</li>
<li>离散型变量交叉熵的计算：$H(p,q)=\sum\limits_&amp;#123x&amp;#125&amp;#123p(x)\log \left( \frac&amp;#1231&amp;#125&amp;#123q(x)&amp;#125 \right)&amp;#125$</li>
<li>连续型变量交叉熵的计算：$-\int_&amp;#123X&amp;#125&amp;#123P(x)\log Q(x)dr(x)=&amp;#123&amp;#123E&amp;#125_&amp;#123p&amp;#125&amp;#125\left[ -\log Q \right]&amp;#125$</li>
</ul>
<h2 id="结构化概率模型"><a href="#结构化概率模型" class="headerlink" title="结构化概率模型"></a>结构化概率模型</h2><p>我们可以把概率分布分解成许多因子的乘积形式。比如：<br>$$p(a,b,c)=p(a)p(b|a)p(c|b)$$<br>（变量之间关系…右边很明了，不再赘述）<br>这种形式能极大的降低表示联合概率分布的成本，当用图来表示这种概率分布的分解时，我们把它称为<strong>结构化概率分布（Structured probabilistic model）</strong> 或者 <strong>图模型（graphical model）</strong></p>
<ul>
<li><strong>有向（directed）</strong> 模型 使用带有有向边的图，他们用条件分布来表示分解。有向模型对于分布中的每一个随机变量$x_i$都包含着一个影响因子，这个组成$x_i$条件概率的影响因子被称为$x_i$的父节点，记为$&amp;#123&amp;#123P&amp;#125_&amp;#123a\varsigma &amp;#125&amp;#125(&amp;#123&amp;#123x&amp;#125_&amp;#123i&amp;#125&amp;#125)$。<br>$$p(x)=\prod\limits_&amp;#123i&amp;#125&amp;#123p\left( &amp;#123&amp;#123x&amp;#125_&amp;#123i&amp;#125&amp;#125|&amp;#123&amp;#123P&amp;#125_&amp;#123a\varsigma &amp;#125&amp;#125(&amp;#123&amp;#123x&amp;#125_&amp;#123i&amp;#125&amp;#125) \right)&amp;#125$$</li>
</ul>
<ul>
<li><strong>无向（undirected）</strong> 模型<br>无向模型使用带有无向边的图，它们将分解表示成一组函数：不像有有向模型那样，这些函数通常不是任何类型的概率分布。$&amp;#123\varsigma&amp;#125$ 中任何满足两两之间有边连接的顶点的集合被称为团。无向模型中的每个团$&amp;#123&amp;#123C&amp;#125^&amp;#123(i)&amp;#125&amp;#125$ 都伴随着一个因子$&amp;#123&amp;#123\phi &amp;#125^&amp;#123(i)&amp;#125&amp;#125\left( &amp;#123&amp;#123C&amp;#125^&amp;#123(i)&amp;#125&amp;#125 \right)$ 。这些因子仅仅是函数，并不是概率分布。每个因子的输出都必须是非负的，但是并没有像概率分布中那样要求因子的和或者积分为1.<br>随机变量的联合概率与所有这些因子的乘积<strong>成比例（proportional）</strong> ——这意味着因子的值越大，则可能性越大。当然，不能保证这种乘积的求和为1。所有我们需要除以一个归一化常数$Z$来得到归一化的概率分布，归一化常数$Z$被定义为 $\phi$函数乘积的所有状态的求和或积分。概率分布为：<br>$$p(x)=\frac&amp;#1231&amp;#125&amp;#123Z&amp;#125\prod\limits_&amp;#123i&amp;#125&amp;#123&amp;#123&amp;#123\phi &amp;#125^&amp;#123(i)&amp;#125&amp;#125\left( &amp;#123&amp;#123C&amp;#125^&amp;#123(i)&amp;#125&amp;#125 \right)&amp;#125$$</li>
</ul>
<p>有向或者无向不是概率分布的特征；它是概率分布的一种特殊<strong>描述（description）</strong> 所具有的特性，而<strong>任何概率分布都可以用这两种方式进行描述。</strong></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/27/《Deap-learning》阅读学习要点笔记（第一部分：应用数学与机器学习基础）——第二章：线性代数/" rel="next" title="《Deap learning》阅读学习要点笔记（第一部分：应用数学与机器学习基础）——第二章：线性代数">
                <i class="fa fa-chevron-left"></i> 《Deap learning》阅读学习要点笔记（第一部分：应用数学与机器学习基础）——第二章：线性代数
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Lau James</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/LauJames" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:LauJames_work@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-为什么要使用概率"><span class="nav-number">1.</span> <span class="nav-text">3.1 为什么要使用概率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-9-常用概率分布"><span class="nav-number">2.</span> <span class="nav-text">3.9 常用概率分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-10-常用函数的有用性质"><span class="nav-number">3.</span> <span class="nav-text">3.10 常用函数的有用性质</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-11-贝叶斯规则"><span class="nav-number">4.</span> <span class="nav-text">3.11 贝叶斯规则</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-13-信息论"><span class="nav-number">5.</span> <span class="nav-text">3.13 信息论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#结构化概率模型"><span class="nav-number">6.</span> <span class="nav-text">结构化概率模型</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lau James</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
<script type="text/javascript" src="js/src/love.js"></script>
